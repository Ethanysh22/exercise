## 诗歌生成作业报告

2152199 余苏皓

### RNN模型

RNN结构：

RNN的结构是由一个输入层、隐藏层、输出层组成：

![img](https://pic1.zhimg.com/v2-b8a6c264b2275569550ef1d88b09da4c_r.jpg)

将RNN的结构按照时间序列展开

![img](https://pic3.zhimg.com/80/v2-3a38f14095f31ccea0e5ee601fd5a1f6_1440w.webp)

对应的前向传播公式和对应的每个时刻的输出公式
$$
S_{t-1} = U_{t-1}X_{t-1} + W_{t-1}S_{t-2}+b_1 \quad y_{t-1} = V_{t-1}S_{t-1}+b_2\\
S_{t} = U_{t}X_{t} + W_{t}S_{t-1}+b_1 \quad y_{t-1} = V_{t}S_{t}+b_2\\
S_{t+1} = U_{t+1}X_{t+1} + W_{t+1}S_{t}+b_1 \quad y_{t+1} = V_{t+1}S_{t+1}+b_2
$$

### LSTM（Long Short-Term Memory，长短期记忆网络）

LSTM是一种特殊的RNN类型，一般的RNN结构如下图所示，是一种将以往学习的结果应用到当前学习的模型，但是这种一般的RNN存在着许多的弊端。如果相关的信息和预测的词位置之间的间隔是非常小，RNN 可以学会使用先前的信息。

![img](https://pic2.zhimg.com/80/v2-351dada05479d11a522776d088407691_1440w.webp)

![img](https://pic1.zhimg.com/80/v2-467c9e7a846b79bd6a21081c195c9234_1440w.webp)

但在比较长的环境中，RNN 并不能够成功学习到这些知识。然而，LSTM模型就可以解决这一问题.

![img](https://pic4.zhimg.com/80/v2-57d12824c92173c9170077a68210fdab_1440w.webp)

如图所示，标准LSTM模型是一种特殊的RNN类型，在每一个重复的模块中有四个特殊的结构，以一种特殊的方式进行交互。在图中，每一条黑线传输着一整个向量，粉色的圈代表一种pointwise 操作(将定义域上的每一点的函数值分别进行运算)，诸如向量的和，而黄色的矩阵就是学习到的神经网络层。
LSTM模型的核心思想是“细胞状态”。“细胞状态”类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流传保持不变会很容易。

![img](https://pic2.zhimg.com/80/v2-ec7398c5980019a9dac829cd4a905d2d_1440w.webp)

LSTM 有通过精心设计的称作为“门”的结构来去除或者增加信息到细胞状态的能力。门是一种让信息选择式通过的方法。他们包含一个 sigmoid 神经网络层和一个 pointwise 乘法操作。

![img](https://pic3.zhimg.com/80/v2-835e18c5697bbcda6c564864450b373e_1440w.webp)

Sigmoid 层输出 0 到 1 之间的数值，描述每个部分有多少量可以通过。0 代表“不许任何量通过”，1 就指“允许任意量通过”。LSTM 拥有三个门，来保护和控制细胞状态。

![img](https://pic3.zhimg.com/80/v2-ffbb02d14e4a77d02f41e80f2a535e2a_1440w.webp)

在LSTM模型中，第一步是决定我们从“细胞”中丢弃什么信息，这个操作由一个忘记门层来完成。该层读取当前输入x和前神经元信息h，由ft来决定丢弃的信息。输出结果1表示“完全保留”，0 表示“完全舍弃”。

![img](https://pic4.zhimg.com/80/v2-27ce136f6977dc85d78cf86689647893_1440w.webp)

第二步是确定细胞状态所存放的新信息，这一步由两层组成。sigmoid层作为“输入门层”，决定我们将要更新的值i；tanh层来创建一个新的候选值向量$\widetilde C_t$ 加入到状态中。在语言模型的例子中，我们希望增加新的主语到细胞状态中，来替代旧的需要忘记的主语。

![img](https://pic3.zhimg.com/80/v2-3c5c69fe61f978a330fc9103800eed86_1440w.webp)

第三步就是更新旧细胞的状态，将$C_{t-1}$更新为 $C_{t}$ 。我们把旧状态与 $f_{t}$ 相乘，丢弃掉我们确定需要丢弃的信息。接着加上$i_t * \widetilde C_{t-1}$。这就是新的候选值，根据我们决定更新每个状态的程度进行变化。在语言模型的例子中，这就是我们实际根据前面确定的目标，丢弃旧代词的信息并添加新的信息的地方

![img](https://pic2.zhimg.com/80/v2-f684adc8040a9a07159657974ff45fc1_1440w.webp)

最后一步就是确定输出了，这个输出将会基于我们的细胞状态，但是也是一个过滤后的版本。首先，我们运行一个 sigmoid 层来确定细胞状态的哪个部分将输出出去。接着，我们把细胞状态通过 tanh 进行处理（得到一个在 -1 到 1 之间的值）并将它和 sigmoid 门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。在语言模型的例子中，因为语境中有一个代词，可能需要输出与之相关的信息。例如，输出判断是一个动词，那么我们需要根据代词是单数还是负数，进行动词的词形变化。

### GRU（Gated Recurrent Unit, LSTM变体）

![img](https://pic1.zhimg.com/80/v2-1838ebd696f1e4d16e41f1a126ff85a0_1440w.webp)

GRU作为LSTM的一种变体，将忘记门和输入门合成了一个单一的更新门。同样还混合了细胞状态和隐藏状态，加诸其他一些改动。最终的模型比标准的 LSTM 模型要简单，也是非常流行的变体。

---

### 诗歌生成过程

以tensorflow版本为例

#### 初始化状态

- `state` 初始化为两个随机正态分布的向量，每个向量的形状为 `(1, 128)`，标准差为 `0.5`。这些向量代表了模型的初始隐藏状态。
- `cur_token` 初始化为 `bos`（beginning of sentence，句子开头）标记的ID，这是生成过程的起点。

#### 生成循环

- 通过一个最多迭代50次的循环来生成词汇。每次迭代尝试添加一个新的词汇到句子中。

#### 生成下一个词

- 在每次迭代中，当前的词汇 `cur_token` 和当前的状态 `state` 被传递给 `get_next_token` 函数，以预测下一个词的ID并更新状态。
- `get_next_token` 函数内部，当前词汇通过嵌入层转换成词嵌入向量。然后，这个嵌入向量和当前的状态一起被传递给RNN单元（`rnncell.call`），产生新的状态和一个输出向量 `h`。
- 输出向量 `h` 通过一个全连接层（`dense`）转换成对词汇表中每个词的打分（`logits`）。
- 使用 `tf.argmax` 从这些打分中选择最高的，作为下一个词的ID。

#### 构建句子

- 如果生成的词是 `eos`（end of sentence，句子结束）标记，则继续下一次迭代而不将其添加到收集器 `collect` 中。这是因为你希望生成的句子在遇到句子结束标记前不停止。
- 如果生成的词不是 `eos`，则将其ID添加到 `collect` 列表中。
- 迭代完成后，通过 `collect` 列表中的ID，使用 `id2word` 字典将每个词的ID转换成对应的词。
- 将这些词连接成一个字符串，形成最终的生成文本。

#### 输出

- 打印出通过上述过程生成的文本。

这个过程展示了一个简化的诗歌生成模型，其中使用了RNN和词嵌入来根据当前状态和已生成的词序列预测下一个最可能的词。

---

### 训练截图

![image-20240327000510508](C:\Users\EthanYu\AppData\Roaming\Typora\typora-user-images\image-20240327000510508.png)

![image-20240327091857026](C:\Users\EthanYu\AppData\Roaming\Typora\typora-user-images\image-20240327091857026.png)



![image-20240327113504462](C:\Users\EthanYu\AppData\Roaming\Typora\typora-user-images\image-20240327113504462.png)

---

### 实验总结

在本次实验中，我们分别使用TensorFlow和PyTorch框架实现了基于RNN和LSTM的模型，旨在解决诗歌生成和加法进位两个问题。TensorFlow实现的是一个基本的循环神经网络（RNN），而PyTorch实现则采用了长短期记忆网络（LSTM）。

对于诗歌生成任务，RNN模型能够捕捉到文本数据中的时间序列依赖性，生成具有一定韵律和意境的文本片段。然而，由于RNN的梯度消失问题，模型在捕捉长距离依赖时表现不佳。相比之下，LSTM模型通过引入遗忘门、输入门和输出门，有效地解决了梯度消失问题，表现出更好的长距离序列学习能力，从而能生成更加连贯和有深度的诗歌文本。

总体而言，尽管RNN在某些情况下能够给出满意的结果，LSTM由于其结构上的改进，无论是在理解复杂的语言模式还是处理具有复杂依赖关系的数学问题上，都显示出更强大的性能和更广泛的适用性。这次实验强调了在处理具有长距离依赖特性的序列数据时，选择合适的模型架构的重要性。